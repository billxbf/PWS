{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of TriviaQA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nodes.Worker\n",
    "\n",
    "with open(\"/home/billxbf/Documents/myks/openai.key\", \"r\") as f:\n",
    "    keys = f.readlines()\n",
    "    os.environ[\"OPENAI_API_KEY\"] = keys[0].strip()\n",
    "with open(\"/home/billxbf/Documents/myks/serpapi.key\", \"r\") as f:\n",
    "    keys = f.readlines()\n",
    "    os.environ[\"SERPAPI_API_KEY\"] = keys[0].strip()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T04:14:47.843352Z",
     "end_time": "2023-04-28T04:14:48.177098Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from utils.DataLoader import DataLoader\n",
    "from utils.Evaluator import Evaluator\n",
    "from algos.PWS import *\n",
    "from algos.react import ReactBase\n",
    "from algos.notool import IO, CoT\n",
    "from prompts import fewshots"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T04:14:48.911744Z",
     "end_time": "2023-04-28T04:14:49.172663Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def save_data(dataset, data, save_path):\n",
    "    dataset[\"preds\"] = data[\"preds\"]\n",
    "    dataset[\"em\"]  = data[\"em\"]\n",
    "    dataset[\"f1\"] = data[\"f1\"]\n",
    "    dataset[\"acc\"] = data[\"acc\"]\n",
    "    dataset[\"wall_time\"] = data[\"wall_time\"]\n",
    "    dataset[\"total_tokens\"] = data[\"total_tokens\"]\n",
    "    dataset[\"steps\"] = data[\"steps\"]\n",
    "    dataset[\"tool_cost\"] = data[\"tool_cost\"]\n",
    "    dataset[\"token_cost\"] = data[\"token_cost\"]\n",
    "    dataset[\"total_cost\"] = data[\"total_cost\"]\n",
    "    dataset.to_csv(save_path, index=False)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T04:14:49.172234Z",
     "end_time": "2023-04-28T04:14:49.173239Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "EVAL_LLM = \"gpt-3.5-turbo\"\n",
    "EVAL_DATASET = \"trivia_qa\"\n",
    "SEED = 2024"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T04:14:49.300815Z",
     "end_time": "2023-04-28T04:14:49.348500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standard IO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset trivia_qa (/home/billxbf/workspace/PWS/data/trivia_qa/trivia_qa/rc.nocontext/1.2.0/e73c5e47a8704744fa9ded33504b35a6c098661813d1c2a09892eb9b9e9d59ae)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e735a98503d54010968981b78ae27ce3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = DataLoader(EVAL_DATASET, seed=SEED).load(500)\n",
    "io = IO(model_name=EVAL_LLM)\n",
    "eval = Evaluator(task=EVAL_DATASET, dataset=dataset, algo=io)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T20:32:16.489627Z",
     "end_time": "2023-04-27T20:32:17.560358Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************* Start Evaluation *******************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [21:28<00:00,  2.58s/it] \n"
     ]
    },
    {
     "data": {
      "text/plain": "{'avg_em': 0.642,\n 'avg_f1': 0.7401808725599343,\n 'avg_acc': 0.806,\n 'avg_wall_time': 0.8073866505622864,\n 'avg_total_tokens': 43.466,\n 'avg_total_cost': 8.693200000000001e-05,\n 'avg_steps': 1.0,\n 'avg_token_cost': 8.693200000000001e-05,\n 'avg_tool_cost': 0.0}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, data = eval.run()\n",
    "df = save_data(dataset, data, \"results/triviaqa_io_chat.csv\")\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T20:32:18.446242Z",
     "end_time": "2023-04-27T20:53:47.453780Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            question  \\\n0  \"What was the first name of the character \"\"Ri...   \n1  What seven letter word is the name of the char...   \n2  Which Asian capital city is known as Krung The...   \n3  Jack Bauer is the main character in which TV s...   \n4       Which US singer's real name is Ernest Evans?   \n5  Braxy is a fatal bacterial infection in which ...   \n6  The first coin-operated parking meter in the U...   \n7  What giant bird was hunted to extinction by Mā...   \n8  With which heavy metal pop group is Rick Allen...   \n9  In Sharia law, what is the specific punishment...   \n\n                                              answer  \\\n0  {'aliases': ['Ruperts', 'Rupert', 'RUPERT', 'R...   \n1  {'aliases': ['Snellen (disambiguation)', 'SNEL...   \n2  {'aliases': ['Krung-devamahanagara amararatana...   \n3  {'aliases': ['24', 'twenty-four'], 'normalized...   \n4  {'aliases': ['Chubby Checker'], 'normalized_al...   \n5  {'aliases': ['Sheep', 'Ovis aries', 'Domestic ...   \n6  {'aliases': ['one thousand, nine hundred and t...   \n7  {'aliases': ['Emeidae', 'Moaspecies', 'Wingles...   \n8  {'aliases': ['Def leppard let's go', 'Def lepa...   \n9  {'aliases': ['Lapidate', 'Stoned (punishment)'...   \n\n                                               preds     em        f1  acc  \\\n0                                            Rigsby.  False  0.000000    0   \n1                                           Snellen.   True  1.000000    1   \n2                                           Bangkok.   True  1.000000    1   \n3                                                 24   True  1.000000    1   \n4                                    Chubby Checker.   True  1.000000    1   \n5                                             Sheep.   True  1.000000    1   \n6                                              1935.   True  1.000000    1   \n7                                               Moa.   True  1.000000    1   \n8                                       Def Leppard.   True  1.000000    1   \n9  The specific punishment for adultery in Sharia...  False  0.428571    1   \n\n   wall_time  total_tokens  steps  tool_cost  token_cost  total_cost  \n0   1.242188            47      1          0    0.000094    0.000094  \n1   1.000442            48      1          0    0.000096    0.000096  \n2   1.030027            47      1          0    0.000094    0.000094  \n3   1.270002            31      1          0    0.000062    0.000062  \n4   1.023786            33      1          0    0.000066    0.000066  \n5   0.978746            33      1          0    0.000066    0.000066  \n6   0.992506            47      1          0    0.000094    0.000094  \n7   1.031404            38      1          0    0.000076    0.000076  \n8   0.644419            34      1          0    0.000068    0.000068  \n9   1.675104            45      1          0    0.000090    0.000090  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>preds</th>\n      <th>em</th>\n      <th>f1</th>\n      <th>acc</th>\n      <th>wall_time</th>\n      <th>total_tokens</th>\n      <th>steps</th>\n      <th>tool_cost</th>\n      <th>token_cost</th>\n      <th>total_cost</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"What was the first name of the character \"\"Ri...</td>\n      <td>{'aliases': ['Ruperts', 'Rupert', 'RUPERT', 'R...</td>\n      <td>Rigsby.</td>\n      <td>False</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>1.242188</td>\n      <td>47</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000094</td>\n      <td>0.000094</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What seven letter word is the name of the char...</td>\n      <td>{'aliases': ['Snellen (disambiguation)', 'SNEL...</td>\n      <td>Snellen.</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1.000442</td>\n      <td>48</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000096</td>\n      <td>0.000096</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Which Asian capital city is known as Krung The...</td>\n      <td>{'aliases': ['Krung-devamahanagara amararatana...</td>\n      <td>Bangkok.</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1.030027</td>\n      <td>47</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000094</td>\n      <td>0.000094</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jack Bauer is the main character in which TV s...</td>\n      <td>{'aliases': ['24', 'twenty-four'], 'normalized...</td>\n      <td>24</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1.270002</td>\n      <td>31</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000062</td>\n      <td>0.000062</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Which US singer's real name is Ernest Evans?</td>\n      <td>{'aliases': ['Chubby Checker'], 'normalized_al...</td>\n      <td>Chubby Checker.</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1.023786</td>\n      <td>33</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000066</td>\n      <td>0.000066</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Braxy is a fatal bacterial infection in which ...</td>\n      <td>{'aliases': ['Sheep', 'Ovis aries', 'Domestic ...</td>\n      <td>Sheep.</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>0.978746</td>\n      <td>33</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000066</td>\n      <td>0.000066</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The first coin-operated parking meter in the U...</td>\n      <td>{'aliases': ['one thousand, nine hundred and t...</td>\n      <td>1935.</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>0.992506</td>\n      <td>47</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000094</td>\n      <td>0.000094</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>What giant bird was hunted to extinction by Mā...</td>\n      <td>{'aliases': ['Emeidae', 'Moaspecies', 'Wingles...</td>\n      <td>Moa.</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>1.031404</td>\n      <td>38</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000076</td>\n      <td>0.000076</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>With which heavy metal pop group is Rick Allen...</td>\n      <td>{'aliases': ['Def leppard let's go', 'Def lepa...</td>\n      <td>Def Leppard.</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>0.644419</td>\n      <td>34</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000068</td>\n      <td>0.000068</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>In Sharia law, what is the specific punishment...</td>\n      <td>{'aliases': ['Lapidate', 'Stoned (punishment)'...</td>\n      <td>The specific punishment for adultery in Sharia...</td>\n      <td>False</td>\n      <td>0.428571</td>\n      <td>1</td>\n      <td>1.675104</td>\n      <td>45</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000090</td>\n      <td>0.000090</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T01:47:08.668449Z",
     "end_time": "2023-04-27T01:47:08.672560Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CoT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset trivia_qa (/home/billxbf/workspace/PWS/data/trivia_qa/trivia_qa/rc.nocontext/1.2.0/e73c5e47a8704744fa9ded33504b35a6c098661813d1c2a09892eb9b9e9d59ae)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2331661f8034b33a852f7f67a198156"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = DataLoader(EVAL_DATASET, seed=SEED).load(500)\n",
    "cot = CoT(fewshot=fewshots.TRIVIAQA_COT, model_name=EVAL_LLM)\n",
    "eval = Evaluator(task=EVAL_DATASET, dataset=dataset, algo=cot)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T21:47:55.225077Z",
     "end_time": "2023-04-27T21:47:56.285837Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************* Start Evaluation *******************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:04:13<00:00,  7.71s/it] \n"
     ]
    },
    {
     "data": {
      "text/plain": "{'avg_em': 0.6,\n 'avg_f1': 0.7173426166426166,\n 'avg_acc': 0.786,\n 'avg_wall_time': 5.3582473263740535,\n 'avg_total_tokens': 199.12,\n 'avg_total_cost': 0.00039824000000000006,\n 'avg_steps': 2.088,\n 'avg_token_cost': 0.00039824000000000006,\n 'avg_tool_cost': 0.0}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, data = eval.run()\n",
    "df = save_data(dataset, data, \"results/trivia_qa_cot_chat.csv\")\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T21:47:57.789548Z",
     "end_time": "2023-04-27T22:52:11.378960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            question  \\\n0  \"What was the first name of the character \"\"Ri...   \n1  What seven letter word is the name of the char...   \n2  Which Asian capital city is known as Krung The...   \n3  Jack Bauer is the main character in which TV s...   \n4       Which US singer's real name is Ernest Evans?   \n\n                                              answer  \\\n0  {'aliases': ['Ruperts', 'Rupert', 'RUPERT', 'R...   \n1  {'aliases': ['Snellen (disambiguation)', 'SNEL...   \n2  {'aliases': ['Krung-devamahanagara amararatana...   \n3  {'aliases': ['24', 'twenty-four'], 'normalized...   \n4  {'aliases': ['Chubby Checker'], 'normalized_al...   \n\n                                          preds     em        f1  acc  \\\n0                                       Rupert.   True  1.000000    1   \n1                                       Snellen   True  1.000000    1   \n2   Bangkok, Thailand on the Chao Phraya River.  False  0.285714    1   \n3                                            24   True  1.000000    1   \n4                                Chubby Checker   True  1.000000    1   \n\n   wall_time  total_tokens  steps  tool_cost  token_cost  total_cost  \n0   6.099105           225      3          0    0.000450    0.000450  \n1   5.031381           193      2          0    0.000386    0.000386  \n2   5.260989           206      2          0    0.000412    0.000412  \n3   2.252567           147      1          0    0.000294    0.000294  \n4   4.328381           177      2          0    0.000354    0.000354  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>preds</th>\n      <th>em</th>\n      <th>f1</th>\n      <th>acc</th>\n      <th>wall_time</th>\n      <th>total_tokens</th>\n      <th>steps</th>\n      <th>tool_cost</th>\n      <th>token_cost</th>\n      <th>total_cost</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"What was the first name of the character \"\"Ri...</td>\n      <td>{'aliases': ['Ruperts', 'Rupert', 'RUPERT', 'R...</td>\n      <td>Rupert.</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>6.099105</td>\n      <td>225</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0.000450</td>\n      <td>0.000450</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What seven letter word is the name of the char...</td>\n      <td>{'aliases': ['Snellen (disambiguation)', 'SNEL...</td>\n      <td>Snellen</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>5.031381</td>\n      <td>193</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.000386</td>\n      <td>0.000386</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Which Asian capital city is known as Krung The...</td>\n      <td>{'aliases': ['Krung-devamahanagara amararatana...</td>\n      <td>Bangkok, Thailand on the Chao Phraya River.</td>\n      <td>False</td>\n      <td>0.285714</td>\n      <td>1</td>\n      <td>5.260989</td>\n      <td>206</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.000412</td>\n      <td>0.000412</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jack Bauer is the main character in which TV s...</td>\n      <td>{'aliases': ['24', 'twenty-four'], 'normalized...</td>\n      <td>24</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>2.252567</td>\n      <td>147</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000294</td>\n      <td>0.000294</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Which US singer's real name is Ernest Evans?</td>\n      <td>{'aliases': ['Chubby Checker'], 'normalized_al...</td>\n      <td>Chubby Checker</td>\n      <td>True</td>\n      <td>1.000000</td>\n      <td>1</td>\n      <td>4.328381</td>\n      <td>177</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.000354</td>\n      <td>0.000354</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T22:52:11.380345Z",
     "end_time": "2023-04-27T22:52:11.381861Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'Bakewell is a market town and civil parish in the Derbyshire Dales district of Derbyshire, England, known for Bakewell pudding. It lies on the River Wye, 13 miles (21 km) south-west of Sheffield. At the 2011 census, the population of the civil parish was 3,949. It was estimated at 3,695 in 2019. The town is close to the tourist attractions of Chatsworth House and Haddon Hall.'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nodes.Worker import WikipediaWorker\n",
    "work = WikipediaWorker()\n",
    "work.run(\"Bakewell\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T01:22:42.234319Z",
     "end_time": "2023-04-28T01:22:42.923602Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types:\n",
      "(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.\n",
      "(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.\n",
      "(3) Finish[answer], which returns the answer and finishes the task.\n",
      "Here are some examples.\n",
      "\n",
      "Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
      "Thought: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
      "Action: Search[Colorado orogeny]\n",
      "Observation: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
      "Thought: It does not mention the eastern sector. So I need to look up eastern sector.\n",
      "Action: Lookup[eastern sector]\n",
      "Observation: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
      "Thought: The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
      "Action: Search[High Plains]\n",
      "Observation: High Plains refers to one of two distinct land regions\n",
      "Thought: I need to instead search High Plains (United States).\n",
      "Action: Search[High Plains (United States)]\n",
      "Observation: The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\n",
      "Thought: High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
      "Action: Finish[1,800 to 7,000 ft]\n",
      "\n",
      "Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
      "Thought: The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
      "Action: Search[Milhouse]\n",
      "Observation: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
      "Thought: The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
      "Action: Lookup[named after]\n",
      "Observation: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
      "Thought: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
      "Action: Finish[Richard Nixon]\n",
      "\n",
      "Question: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\n",
      "Thought: I need to search Adam Clayton Powell and The Saimaa Gesture, and find which documentary is about Finnish rock groups.\n",
      "Action: Search[Adam Clayton Powell]\n",
      "Observation: Could not find [Adam Clayton Powell]. Similar: [’Adam Clayton Powell III’, ’Seventh Avenue (Manhattan)’, ’Adam Clayton Powell Jr. State Office Building’, ’Isabel Washington Powell’, ’Adam Powell’, ’Adam Clayton Powell (film)’, ’Giancarlo Esposito’].\n",
      "Thought: To find the documentary, I can search Adam Clayton Powell (film).\n",
      "Action: Search[Adam Clayton Powell (film)]\n",
      "Observation: Adam Clayton Powell is a 1989 American documentary film directed by Richard Kilberg. The film is about the rise and fall of influential African-American politician Adam Clayton Powell Jr.[3][4] It was later aired as part of the PBS series The American Experience.\n",
      "Thought: Adam Clayton Powell (film) is a documentary about an African-American politician, not Finnish rock groups. So the documentary about Finnish rock groups must instead be The Saimaa Gesture.\n",
      "Action: Finish[The Saimaa Gesture]\n",
      "\n",
      "Question: What profession does Nicholas Ray and Elia Kazan have in common?\n",
      "Thought: I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.\n",
      "Action: Search[Nicholas Ray]\n",
      "Observation: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\n",
      "Thought: Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\n",
      "Action: Search[Elia Kazan]\n",
      "Observation: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\n",
      "Thought: Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\n",
      "Action: Finish[director, screenwriter, actor]\n",
      "\n",
      "Question: Which magazine was started first Arthur’s Magazine or First for Women?\n",
      "Thought: I need to search Arthur’s Magazine and First for Women, and find which was started first.\n",
      "Action: Search[Arthur’s Magazine]\n",
      "Observation: Arthur’s Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\n",
      "Thought: Arthur’s Magazine was started in 1844. I need to search First for Women next.\n",
      "Action: Search[First for Women]\n",
      "Observation: First for Women is a woman’s magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\n",
      "Thought: First for Women was started in 1989. 1844 (Arthur’s Magazine) < 1989 (First for Women), so Arthur’s Magazine was started first.\n",
      "Action: Finish[Arthur’s Magazine]\n",
      "\n",
      "Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n",
      "Thought: I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.\n",
      "Action: Search[Pavel Urysohn]\n",
      "Observation: Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\n",
      "Thought: Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.\n",
      "Action: Search[Leonid Levin]\n",
      "Observation: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\n",
      "Thought: Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\n",
      "Action: Finish[yes]\n",
      "\n",
      "\n",
      "Question: {input}\n",
      "{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(fewshots.HOTPOTQA_REACT)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T01:20:56.609279Z",
     "end_time": "2023-04-28T01:20:56.655988Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## REACT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset trivia_qa (/home/billxbf/workspace/PWS/data/trivia_qa/trivia_qa/rc.nocontext/1.2.0/e73c5e47a8704744fa9ded33504b35a6c098661813d1c2a09892eb9b9e9d59ae)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14fbb7350a3445ef867fe22fd72c38fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = DataLoader(EVAL_DATASET, seed=SEED).load(50)\n",
    "react = ReactBase(fewshot=fewshots.TRIVIAQA_REACT, model_name=EVAL_LLM, max_iter=8, verbose=False)\n",
    "eval = Evaluator(task=EVAL_DATASET, dataset=dataset, algo=react)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T01:34:41.504220Z",
     "end_time": "2023-04-28T01:34:42.758505Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset trivia_qa (/home/billxbf/workspace/PWS/data/trivia_qa/trivia_qa/rc.nocontext/1.2.0/e73c5e47a8704744fa9ded33504b35a6c098661813d1c2a09892eb9b9e9d59ae)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca60f656835846d5a094bcf94c89378f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************* Start Evaluation *******************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/billxbf/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/billxbf/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "100%|██████████| 50/50 [14:26<00:00, 17.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'avg_em': 0.48,\n 'avg_f1': 0.5696666666666667,\n 'avg_acc': 0.6,\n 'avg_wall_time': 13.405489727854729,\n 'avg_total_tokens': 3476.2291666666665,\n 'avg_total_cost': 0.006952458333333334,\n 'avg_steps': 4.708333333333333,\n 'avg_token_cost': 0.006952458333333334,\n 'avg_tool_cost': 0.0}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DataLoader(EVAL_DATASET, seed=111).load(50)\n",
    "react = ReactBase(fewshot=fewshots.TRIVIAQA_REACT, model_name=EVAL_LLM, max_iter=8, verbose=False)\n",
    "eval = Evaluator(task=EVAL_DATASET, dataset=dataset, algo=react)\n",
    "response, data = eval.run()\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T01:59:45.938746Z",
     "end_time": "2023-04-28T02:14:13.415001Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************* Start Evaluation *******************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/billxbf/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/billxbf/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "100%|██████████| 50/50 [18:40<00:00, 22.41s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'avg_em': 0.38,\n 'avg_f1': 0.4146666666666667,\n 'avg_acc': 0.48,\n 'avg_wall_time': 15.643525327954974,\n 'avg_total_tokens': 4949.65306122449,\n 'avg_total_cost': 0.00989930612244898,\n 'avg_steps': 5.714285714285714,\n 'avg_token_cost': 0.00989930612244898,\n 'avg_tool_cost': 0.0}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, data = eval.run()\n",
    "df = save_data(dataset, data, \"results/trivia_qa_react_chat.csv\")\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T01:34:46.893110Z",
     "end_time": "2023-04-28T01:53:27.591512Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            question  \\\n0  \"What was the first name of the character \"\"Ri...   \n1  What seven letter word is the name of the char...   \n2  Which Asian capital city is known as Krung The...   \n3  Jack Bauer is the main character in which TV s...   \n4       Which US singer's real name is Ernest Evans?   \n\n                                              answer  \\\n0  {'aliases': ['Ruperts', 'Rupert', 'RUPERT', 'R...   \n1  {'aliases': ['Snellen (disambiguation)', 'SNEL...   \n2  {'aliases': ['Krung-devamahanagara amararatana...   \n3  {'aliases': ['24', 'twenty-four'], 'normalized...   \n4  {'aliases': ['Chubby Checker'], 'normalized_al...   \n\n                                               preds     em   f1  acc  \\\n0  Agent stopped due to iteration limit or time l...  False  0.0    0   \n1  Agent stopped due to iteration limit or time l...  False  0.0    0   \n2                                            Bangkok   True  1.0    1   \n3                                                 24   True  1.0    1   \n4                                       Ernest Evans  False  0.0    0   \n\n   wall_time  total_tokens  steps  tool_cost  token_cost  total_cost  \n0  23.220037        5676.0    9.0        0.0    0.011352    0.011352  \n1  22.953820       13692.0    9.0        0.0    0.027384    0.027384  \n2   5.303156        1312.0    2.0        0.0    0.002624    0.002624  \n3   5.014344        1026.0    2.0        0.0    0.002052    0.002052  \n4  10.034224        2044.0    4.0        0.0    0.004088    0.004088  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>preds</th>\n      <th>em</th>\n      <th>f1</th>\n      <th>acc</th>\n      <th>wall_time</th>\n      <th>total_tokens</th>\n      <th>steps</th>\n      <th>tool_cost</th>\n      <th>token_cost</th>\n      <th>total_cost</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"What was the first name of the character \"\"Ri...</td>\n      <td>{'aliases': ['Ruperts', 'Rupert', 'RUPERT', 'R...</td>\n      <td>Agent stopped due to iteration limit or time l...</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>23.220037</td>\n      <td>5676.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.011352</td>\n      <td>0.011352</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What seven letter word is the name of the char...</td>\n      <td>{'aliases': ['Snellen (disambiguation)', 'SNEL...</td>\n      <td>Agent stopped due to iteration limit or time l...</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>22.953820</td>\n      <td>13692.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>0.027384</td>\n      <td>0.027384</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Which Asian capital city is known as Krung The...</td>\n      <td>{'aliases': ['Krung-devamahanagara amararatana...</td>\n      <td>Bangkok</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>5.303156</td>\n      <td>1312.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.002624</td>\n      <td>0.002624</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jack Bauer is the main character in which TV s...</td>\n      <td>{'aliases': ['24', 'twenty-four'], 'normalized...</td>\n      <td>24</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>5.014344</td>\n      <td>1026.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.002052</td>\n      <td>0.002052</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Which US singer's real name is Ernest Evans?</td>\n      <td>{'aliases': ['Chubby Checker'], 'normalized_al...</td>\n      <td>Ernest Evans</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>10.034224</td>\n      <td>2044.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.004088</td>\n      <td>0.004088</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T01:53:27.591404Z",
     "end_time": "2023-04-28T01:53:27.591599Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PWSBase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset trivia_qa (/home/billxbf/workspace/PWS/data/trivia_qa/trivia_qa/rc.nocontext/1.2.0/e73c5e47a8704744fa9ded33504b35a6c098661813d1c2a09892eb9b9e9d59ae)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42f1294c4d784e369876f3b32051714b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = DataLoader(EVAL_DATASET, seed=SEED).load(500)\n",
    "pwsbase = PWS_Base(fewshot=fewshots.TRIVIAQA_PWS, planner_model=EVAL_LLM, solver_model=EVAL_LLM)\n",
    "eval = Evaluator(task=EVAL_DATASET, dataset=dataset, algo=pwsbase)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T04:17:59.238958Z",
     "end_time": "2023-04-28T04:18:00.354641Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************* Start Evaluation *******************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/500 [00:54<1:52:55, 13.66s/it]/home/billxbf/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/billxbf/anaconda3/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "100%|██████████| 500/500 [2:05:51<00:00, 15.10s/it]  \n"
     ]
    },
    {
     "data": {
      "text/plain": "{'avg_em': 0.518,\n 'avg_f1': 0.6063827217227217,\n 'avg_acc': 0.666,\n 'avg_wall_time': 10.256969241622071,\n 'avg_total_tokens': 1340.9378881987577,\n 'avg_total_cost': 0.009595552795031055,\n 'avg_steps': 3.546583850931677,\n 'avg_token_cost': 0.009595552795031055,\n 'avg_tool_cost': 0.0}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, data = eval.run()\n",
    "df = save_data(dataset, data, \"results/trivia_qa_pwsbase_chat.csv\")\n",
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T04:18:12.202117Z",
     "end_time": "2023-04-28T06:24:03.768420Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            question  \\\n0  \"What was the first name of the character \"\"Ri...   \n1  What seven letter word is the name of the char...   \n2  Which Asian capital city is known as Krung The...   \n3  Jack Bauer is the main character in which TV s...   \n4       Which US singer's real name is Ernest Evans?   \n\n                                              answer  \\\n0  {'aliases': ['Ruperts', 'Rupert', 'RUPERT', 'R...   \n1  {'aliases': ['Snellen (disambiguation)', 'SNEL...   \n2  {'aliases': ['Krung-devamahanagara amararatana...   \n3  {'aliases': ['24', 'twenty-four'], 'normalized...   \n4  {'aliases': ['Chubby Checker'], 'normalized_al...   \n\n                                               preds     em   f1  acc  \\\n0                                           Francis.  False  0.0    0   \n1                                           Snellen.   True  1.0    1   \n2                                           Bangkok.   True  1.0    1   \n3  Jack Bauer is the main character in the TV ser...  False  0.2    1   \n4                                    Chubby Checker.   True  1.0    1   \n\n   wall_time  total_tokens  steps  tool_cost  token_cost  total_cost  \n0  11.308511         972.0    4.0        0.0    0.004734    0.004734  \n1   9.421635        1097.0    3.0        0.0    0.007594    0.007594  \n2  16.723793         544.0    2.0        0.0    0.001574    0.001574  \n3   8.805704        1592.0    4.0        0.0    0.003184    0.003184  \n4   6.603708         651.0    3.0        0.0    0.002688    0.002688  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>preds</th>\n      <th>em</th>\n      <th>f1</th>\n      <th>acc</th>\n      <th>wall_time</th>\n      <th>total_tokens</th>\n      <th>steps</th>\n      <th>tool_cost</th>\n      <th>token_cost</th>\n      <th>total_cost</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\"What was the first name of the character \"\"Ri...</td>\n      <td>{'aliases': ['Ruperts', 'Rupert', 'RUPERT', 'R...</td>\n      <td>Francis.</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>11.308511</td>\n      <td>972.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.004734</td>\n      <td>0.004734</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What seven letter word is the name of the char...</td>\n      <td>{'aliases': ['Snellen (disambiguation)', 'SNEL...</td>\n      <td>Snellen.</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>9.421635</td>\n      <td>1097.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.007594</td>\n      <td>0.007594</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Which Asian capital city is known as Krung The...</td>\n      <td>{'aliases': ['Krung-devamahanagara amararatana...</td>\n      <td>Bangkok.</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>16.723793</td>\n      <td>544.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.001574</td>\n      <td>0.001574</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Jack Bauer is the main character in which TV s...</td>\n      <td>{'aliases': ['24', 'twenty-four'], 'normalized...</td>\n      <td>Jack Bauer is the main character in the TV ser...</td>\n      <td>False</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>8.805704</td>\n      <td>1592.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.003184</td>\n      <td>0.003184</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Which US singer's real name is Ernest Evans?</td>\n      <td>{'aliases': ['Chubby Checker'], 'normalized_al...</td>\n      <td>Chubby Checker.</td>\n      <td>True</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>6.603708</td>\n      <td>651.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.002688</td>\n      <td>0.002688</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T06:24:03.770731Z",
     "end_time": "2023-04-28T06:24:03.775629Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
