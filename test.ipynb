{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "with open(\"/home/billxbf/Documents/myks/openai.key\", \"r\") as f:\n",
    "    keys = f.readlines()\n",
    "    os.environ[\"OPENAI_API_KEY\"] = keys[0].strip()\n",
    "with open(\"/home/billxbf/Documents/myks/serpapi.key\", \"r\") as f:\n",
    "    keys = f.readlines()\n",
    "    os.environ[\"SERPAPI_API_KEY\"] = keys[0].strip()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:36:44.667351Z",
     "end_time": "2023-04-24T00:36:44.667469Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from algos.PWS import *\n",
    "from algos.react import ReactBase, ReactZeroShot\n",
    "from algos.notool import CoT, IO\n",
    "from prompts import fewshots"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:36:44.947402Z",
     "end_time": "2023-04-24T00:36:45.402967Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## REACT BASE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reactbase = ReactBase(fewshot=fewshots.HOTPOTQA_REACT,model_name=\"gpt-3.5-turbo\",verbose=True)\n",
    "reactbase.agent.max_iterations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T21:38:51.400017Z",
     "end_time": "2023-04-21T21:38:51.465631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mThought: I need to search for The Hennchata and find out which cognac house makes the main ingredient.\n",
      "Action: Search[The Hennchata]\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3mThe Hennchata is a cocktail consisting of Hennessy cognac and Mexican rice horchata agua fresca. It was invented in 2013 by Jorge Sánchez at his Chaco's Mexican restaurant in San Jose, California.\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mThe Hennchata contains Hennessy cognac, so I need to search for the cognac house Hennessy and confirm if it is the one that makes the main ingredient.\n",
      "Action: Search[Hennessy cognac]\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3mJas Hennessy & Co., commonly known simply as Hennessy (French pronunciation: ​[ɛnɛsi]), is a French producer of cognac, which has its headquarters in Cognac, France.\n",
      "It is one of the \"big four\" cognac houses, along with Martell, Courvoisier, and Rémy Martin, who together make around 85% of the world's cognac. Hennessy sells approximately 102 million bottles of its cognacs per year, making it the world's largest cognac producer, and in 2017 its sales represented around 60% of the US cognac market. As well as distilling cognac eaux-de-vie itself, the company also acts as a negociant.The brand is owned by Moët Hennessy since a champagne & cognac merger in the early seventies, which is in turn owned by LVMH (66%) and Diageo (34%), Diageo acting as a non-controlling shareholder. Hennessy pioneered several industry-standard practices in the world of cognac, and its association with luxury has made it a regular point of reference in popular culture, especially in hip hop.\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mHennessy is the cognac house that makes the main ingredient in The Hennchata. \n",
      "Action: Finish[Hennessy]\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "response = reactbase.run(\"What is the name of the cognac house that makes the main ingredient in The Hennchata?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T21:38:53.821711Z",
     "end_time": "2023-04-21T21:39:02.681565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{'wall_time': 8.863192796707153,\n 'input': 'What is the name of the cognac house that makes the main ingredient in The Hennchata?',\n 'output': 'Hennessy',\n 'intermediate_steps': [(AgentAction(tool='Search', tool_input='The Hennchata', log='Thought: I need to search for The Hennchata and find out which cognac house makes the main ingredient.\\nAction: Search[The Hennchata]'),\n   \"The Hennchata is a cocktail consisting of Hennessy cognac and Mexican rice horchata agua fresca. It was invented in 2013 by Jorge Sánchez at his Chaco's Mexican restaurant in San Jose, California.\"),\n  (AgentAction(tool='Search', tool_input='Hennessy cognac', log='The Hennchata contains Hennessy cognac, so I need to search for the cognac house Hennessy and confirm if it is the one that makes the main ingredient.\\nAction: Search[Hennessy cognac]'),\n   'Jas Hennessy & Co., commonly known simply as Hennessy (French pronunciation: \\u200b[ɛnɛsi]), is a French producer of cognac, which has its headquarters in Cognac, France.\\nIt is one of the \"big four\" cognac houses, along with Martell, Courvoisier, and Rémy Martin, who together make around 85% of the world\\'s cognac. Hennessy sells approximately 102 million bottles of its cognacs per year, making it the world\\'s largest cognac producer, and in 2017 its sales represented around 60% of the US cognac market. As well as distilling cognac eaux-de-vie itself, the company also acts as a negociant.The brand is owned by Moët Hennessy since a champagne & cognac merger in the early seventies, which is in turn owned by LVMH (66%) and Diageo (34%), Diageo acting as a non-controlling shareholder. Hennessy pioneered several industry-standard practices in the world of cognac, and its association with luxury has made it a regular point of reference in popular culture, especially in hip hop.')],\n 'tool_usage': {'search': 2, 'lookup': 0},\n 'total_tokens': 4764,\n 'prompt_tokens': 4653,\n 'completion_tokens': 111,\n 'total_cost': 0.009528,\n 'steps': 3,\n 'token_cost': 0.009528,\n 'tool_cost': 0}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T21:39:07.345794Z",
     "end_time": "2023-04-21T21:39:07.347907Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## REACT ZERO SHOT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ReactZeroShot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_508940/1881762375.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mreactzero\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mReactZeroShot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mresponse2\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreactzero\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Who is the director of the 2003 film which has scenes in it filmed at the Quality Cafe in Los Angeles?\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ReactZeroShot' is not defined"
     ]
    }
   ],
   "source": [
    "reactzero = ReactZeroShot(verbose=True)\n",
    "response2 = reactzero.run(\"Who is the director of the 2003 film which has scenes in it filmed at the Quality Cafe in Los Angeles?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:11:43.280861Z",
     "end_time": "2023-04-17T04:11:50.057706Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'wall_time': 8.320967435836792,\n 'input': 'Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?',\n 'output': 'Richard Nixon',\n 'intermediate_steps': [(AgentAction(tool='Search', tool_input='Matt Groening Milhouse', log=' I need to find out who Matt Groening named Milhouse after.\\nAction: Search\\nAction Input: \"Matt Groening Milhouse\"'),\n   'Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.'),\n  (AgentAction(tool='Search', tool_input='Matt Groening Milhouse origin', log=' I need to find out who Matt Groening named Milhouse after.\\nAction: Search\\nAction Input: \"Matt Groening Milhouse origin\"'),\n   'Creation. Milhouse was designed by Matt Groening for a planned series on NBC, which was abandoned. The design was then used for a Butterfinger commercial, and it was decided to use the character in the series. Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.')],\n 'tool_usage': {'serpapi': 2, 'llm-math_token': 0},\n 'total_tokens': 932,\n 'prompt_tokens': 860,\n 'completion_tokens': 72,\n 'total_cost': 0.03864}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T03:59:47.908567Z",
     "end_time": "2023-04-17T03:59:47.911943Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PWS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "pws = PWS_Base()\n",
    "response3 = pws.run(\"What is the name of the cognac house that makes the main ingredient in The Hennchata?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T21:41:30.079810Z",
     "end_time": "2023-04-21T21:41:37.141476Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'wall_time': 7.061087608337402,\n 'input': 'What is the name of the cognac house that makes the main ingredient in The Hennchata?',\n 'output': 'Hennessy.',\n 'planner_log': \"For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E (#E1, #E2, ...) that can be called by later tools. Available tools are: \\n\\nTools can be one of the following:\\nWikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\\nLLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\\n\\nBegin! Describe your plans with rich details.\\n\\nQuestion: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\\nPlan: Search for more information about Colorado orogeny.\\n#E1 = Wikipedia[Colorado orogeny]\\nPlan: Find out the area that eastern sector of the Colorado orogeny extends into.\\n#E2 = LLM[What is the name of the area that eastern sector of Colorado extends into? Given context: #E1]\\nPlan: Search for more information about the area.\\n#E3 = Wikipedia[#E2]\\nPlan: Find out the elevation range for the area.\\n#E4 = LLM[What is elevation range for the area #E2? Given context: #E3]\\n\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nPlan: Search for more information about Arthur's Magazine.\\n#E1 = Wikipedia[Arthur's Magazine]\\nPlan: Search for more information about First for Women.\\n#E2 = Wikipedia[First for Women]\\nPlan: Compare the two start dates and determine which magazine was started first.\\n#E3 = LLM[Which magazine was started first Arthur's Magazine or First for Women? Given context: #E1, #E2]\\nWhat is the name of the cognac house that makes the main ingredient in The Hennchata?\\nPlan: Search for more information about The Hennchata.\\n#E1 = Wikipedia[The Hennchata]\\nPlan: Find out the name of the cognac house that makes the main ingredient in The Hennchata.\\n#E2 = LLM[What is the name of the cognac house that makes the main ingredient in The Hennchata? Given context: #E1]\",\n 'worker_log': \"Plan: Search for more information about The Hennchata.\\nEvidence:\\nThe Hennchata is a cocktail consisting of Hennessy cognac and Mexican rice horchata agua fresca. It was invented in 2013 by Jorge Sánchez at his Chaco's Mexican restaurant in San Jose, California.\\nPlan: Find out the name of the cognac house that makes the main ingredient in The Hennchata.\\nEvidence:\\nHennessy.\\n\",\n 'solver_log': \"Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\\n\\nWhat is the name of the cognac house that makes the main ingredient in The Hennchata?\\nPlan: Search for more information about The Hennchata.\\nEvidence:\\nThe Hennchata is a cocktail consisting of Hennessy cognac and Mexican rice horchata agua fresca. It was invented in 2013 by Jorge Sánchez at his Chaco's Mexican restaurant in San Jose, California.\\nPlan: Find out the name of the cognac house that makes the main ingredient in The Hennchata.\\nEvidence:\\nHennessy.\\n\\nNow begin to solve the task or problem. Respond directly with no extra words.\\nWhat is the name of the cognac house that makes the main ingredient in The Hennchata?\\nHennessy.\",\n 'tool_usage': {'LLM_token': 77},\n 'steps': 3,\n 'total_tokens': 857,\n 'token_cost': 0.017140000000000002,\n 'tool_cost': 0.0,\n 'total_cost': 0.017140000000000002}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T21:41:59.477974Z",
     "end_time": "2023-04-21T21:41:59.486365Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\n",
      "\n",
      "Are both The New Pornographers and Kings of Leon American rock bands?\n",
      "Plan: Find out if The New Pornographers is an American rock band.\n",
      "Evidence:\n",
      "The New Pornographers are a Canadian indie rock band, formed in 1997 in Vancouver.\n",
      "Plan: Find out if Kings of Leon is an American rock band.\n",
      "Evidence:\n",
      "Kings of Leon is an American rock band formed in Nashville, Tennessee, in 1999. The band is composed of brothers Caleb, Nathan and Jared Followill, and their cousin Matthew Followill.\n",
      "\n",
      "Now begin to solve the task or problem. Repond directly with no extra words.\n",
      "Are both The New Pornographers and Kings of Leon American rock bands?\n",
      "Yes.\n"
     ]
    }
   ],
   "source": [
    "print(response3[\"solver_log\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T04:09:18.035380Z",
     "end_time": "2023-04-17T04:09:18.035682Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nodes.Worker import WikipediaWorker, DocStoreLookUpWorker\n",
    "from utils.CustomDocstoreExplorer import CustomDocstoreExplorer\n",
    "from langchain import Wikipedia\n",
    "docstore = CustomDocstoreExplorer(Wikipedia())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"prompts/prompts_naive.json\") as f:\n",
    "    data = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T01:43:31.661130Z",
     "end_time": "2023-04-17T01:43:31.663047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\\nPlan: Search for more information about Colorado orogeny.\\n#E1 = Wikipedia[Colorado orogeny]\\nPlan: Look up for key information about eastern sector in search result.\\n#E2 = LookUp[eastern sector]\\nPlan: Find out the area that eastern sector of the Colorado orogeny extends into.\\n#E4 = LLM[What is the name of the area that eastern sector of Colorado extends into? Given context: #E2]\\nPlan: Search for more information about the area.\\n#E5 = Wikipedia[#E4]\\nPlan: Look up for the key information about elevation in search result.\\n#E6 = LookUp[elevation]\\nPlan: Find out the elevation range for the area.\\n#E7 = LLM[What is elevation range for the area [#E4]? Given context: #E5]\\n\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nPlan: Search for more information about Arthur's Magazine.\\n#E1 = Wikipedia[Arthur's Magazine] \\nPlan: Look up for key information about founding date in search result.\\n#E2 = LookUp[found] \\nPlan: Search for more information about First for Women.\\n#E3 = Wikipedia[First for Women] \\nPlan: Look up for key information about founding date in search result.\\n#E4 = LookUp[found]\\nPlan: Compare the founding date of the two magazines.\\n#E5 = LLM[Which magazine was started first [Arthur's Magazine] or [First for Women]? Given context: #E2 and #E4]\""
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T03:17:15.996030Z",
     "end_time": "2023-04-17T03:17:16.000651Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\\nPlan: Search for more information about Colorado orogeny.\\n#E1 = Wikipedia[Colorado orogeny]\\nPlan: Look up for key information about eastern sector in search result.\\n#E2 = LookUp[eastern sector]\\nPlan: Find out the area that eastern sector of the Colorado orogeny extends into.\\n#E4 = LLM[What is the name of the area that eastern sector of Colorado extends into? Given context: #E2]\\nPlan: Search for more information about the area.\\n#E5 = Wikipedia[#E4]\\nPlan: Look up for the key information about elevation in search result.\\n#E6 = LookUp[elevation]\\nPlan: Find out the elevation range for the area.\\n#E7 = LLM[What is elevation range for the area [#E4]? Given context: #E5]\\n\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nPlan: Search for more information about Arthur's Magazine.\\n#E1 = Wikipedia[Arthur's Magazine] \\nPlan: Look up for key information about founding date in search result.\\n#E2 = LookUp[found] \\nPlan: Search for more information about First for Women.\\n#E3 = Wikipedia[First for Women] \\nPlan: Look up for key information about founding date in search result.\\n#E4 = LookUp[found]\\nPlan: Compare the founding date of the two magazines.\\n#E5 = LLM[Which magazine was started first [Arthur's Magazine] or [First for Women]? Given context: #E2 and #E4]\\n\""
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompts import fewshots\n",
    "fewshots.HOTPOTQA_PWS_BASE"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T03:32:47.134617Z",
     "end_time": "2023-04-17T03:32:47.140611Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Arthur's Magazine.\""
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nodes.Worker import LLMWorker\n",
    "\n",
    "tool = LLMWorker()\n",
    "tool.run(\"Which magazine was started first Arthur's Magazine or First for Women? \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T03:36:35.359034Z",
     "end_time": "2023-04-17T03:36:35.756787Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E (#E1, #E2, ...) that can be called by later tools. Available tools are: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E (#E1, #E2, ...) that can be called by later tools. Available tools are: \\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-17T03:39:20.933216Z",
     "end_time": "2023-04-17T03:39:20.937524Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CoT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "{'wall_time': 4.111898183822632,\n 'input': \"Answer following questions. Let's think step by step. Give your reasoning process, and then answer the question in a new line directly with no extra words.\\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\\nStep 1: Identify the eastern sector of the Colorado Orogeny\\nThe Colorado Orogeny refers to a series of mountain-building events that occurred in the Rocky Mountains, primarily in the present-day U.S. state of Colorado. The eastern sector of the Colorado Orogeny likely refers to the easternmost part of this mountain range, which includes the Front Range, Wet Mountains, and the Sangre de Cristo Mountains.\\nStep 2: Determine the elevation range of the eastern sector\\nThe Front Range, Wet Mountains, and Sangre de Cristo Mountains have varying elevations. The Front Range has elevations ranging from around 5,000 feet to over 14,000 feet (1,500 meters to 4,300 meters). The Wet Mountains have elevations ranging from about 6,000 feet to over 12,000 feet (1,800 meters to 3,700 meters). The Sangre de Cristo Mountains have elevations ranging from approximately 7,000 feet to over 14,000 feet (2,100 meters to 4,300 meters).\\nStep 3: Combine the elevation ranges\\nConsidering the elevation ranges of the three mountain ranges in the eastern sector of the Colorado Orogeny, the overall elevation range for the area extends from around 5,000 feet (1,500 meters) to over 14,000 feet (4,300 meters).\\nAnswer: 5,000 feet to 4,000 feet\\nWhich magazine was started first Arthur's Magazine or First for Women?\\n\",\n 'output': \"Step 1: Research the magazines\\nArthur's Magazine was a monthly magazine for children that was founded in 1888. First for Women is a women's lifestyle magazine that was founded in 1937.\\nStep 2: Determine which magazine was founded first\\nArthur's Magazine was founded in 1888, while First for Women was founded in 1937.\\n\\nAnswer: Arthur's Magazine\",\n 'prompt_tokens': 358,\n 'completion_tokens': 73,\n 'total_tokens': 431,\n 'total_cost': 0.008620000000000001}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cot = CoT(fewshot=fewshots.HOTPOTQA_COT)\n",
    "cot.run(\"Which magazine was started first Arthur's Magazine or First for Women?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T18:32:20.660413Z",
     "end_time": "2023-04-20T18:32:24.770937Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'wall_time': 0.8477969169616699,\n 'input': 'Answer following questions. Respond directly with no extra words.\\n\\nIn what city and state was the vocalist for  El Grupo Nuevo de Omar Rodriguez Lopez born?\\n',\n 'output': '\\nSan Juan, Puerto Rico',\n 'prompt_tokens': 37,\n 'completion_tokens': 6,\n 'total_tokens': 43,\n 'total_cost': 0.0008600000000000001}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io = IO()\n",
    "io.run('In what city and state was the vocalist for  El Grupo Nuevo de Omar Rodriguez Lopez born?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T19:24:48.664102Z",
     "end_time": "2023-04-20T19:24:49.509555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/21.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e018e856698c4737bdcac0eed064aacb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/LangChainDatasets--question-answering-state-of-the-union to /home/billxbf/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--question-answering-state-of-the-union-a7e5a3b2db4f440d/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a3c62ce03b04cadbb4660382138a9e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/2.90k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7bd43897f46476c89b9abd021890426"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4b0d8ffbe04451f966a8828e3a77929"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97c054175ef646a28c84a9629ed47cef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/billxbf/.cache/huggingface/datasets/LangChainDatasets___json/LangChainDatasets--question-answering-state-of-the-union-a7e5a3b2db4f440d/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f982e45060744245866b2d2c67014f54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.evaluation.loading import load_dataset\n",
    "dataset = load_dataset(\"question-answering-state-of-the-union\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T15:24:26.995752Z",
     "end_time": "2023-04-23T15:24:28.448773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "11"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T15:24:38.065595Z",
     "end_time": "2023-04-23T15:24:38.071091Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from langchain import OpenAI, SQLDatabase, SQLDatabaseChain\n",
    "db = SQLDatabase.from_uri(\"sqlite:///../../../notebooks/Chinook.db\")\n",
    "llm = OpenAI(temperature=0)\n",
    "chain = SQLDatabaseChain(llm=llm, database=db, input_key=\"question\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T14:19:26.132579Z",
     "end_time": "2023-04-23T14:19:26.186632Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## State of the Union"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"data/docs/state_of_the_union.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T15:57:21.095837Z",
     "end_time": "2023-04-23T15:57:21.109089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "vectorstore = VectorstoreIndexCreator().from_loaders([loader]).vectorstore"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T15:57:51.604974Z",
     "end_time": "2023-04-23T15:57:52.932569Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T15:58:25.291000Z",
     "end_time": "2023-04-23T15:58:25.332960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vectorstore.as_retriever(), input_key=\"question\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T15:59:08.305542Z",
     "end_time": "2023-04-23T15:59:08.347426Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "RetrievalQA(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fc265b082e0>, verbose=False, combine_documents_chain=StuffDocumentsChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fc265b082e0>, verbose=False, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fc265b082e0>, verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fc265b082e0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False), output_key='text'), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='context'), input_key='question', output_key='result', return_source_documents=False, retriever=VectorStoreRetriever(vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7fc25ebe85b0>, search_type='similarity', search_kwargs={}))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T15:59:09.975436Z",
     "end_time": "2023-04-23T15:59:09.984262Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "predictions = chain.apply(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T16:01:51.543999Z",
     "end_time": "2023-04-23T16:02:22.393083Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T16:01:34.005042Z",
     "end_time": "2023-04-23T16:01:34.013755Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate(dataset, predictions, question_key=\"question\", prediction_key=\"result\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T16:02:31.510378Z",
     "end_time": "2023-04-23T16:02:32.367103Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "for i, prediction in enumerate(predictions):\n",
    "    prediction['grade'] = graded_outputs[i]['text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T16:02:52.301274Z",
     "end_time": "2023-04-23T16:02:52.343580Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'question': 'What is the purpose of the NATO Alliance?',\n  'answer': 'The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.',\n  'result': ' The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.',\n  'grade': ' CORRECT'},\n {'question': 'What is the U.S. Department of Justice doing to combat the crimes of Russian oligarchs?',\n  'answer': 'The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.',\n  'result': \" The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs. They are also joining with European allies to find and seize the oligarchs' yachts, luxury apartments, and private jets, and going after criminals who stole billions in relief money meant for small businesses and millions of Americans.\",\n  'grade': ' INCORRECT'},\n {'question': 'What is the American Rescue Plan and how did it help Americans?',\n  'answer': 'The American Rescue Plan is a piece of legislation that provided immediate economic relief for tens of millions of Americans. It helped put food on their table, keep a roof over their heads, and cut the cost of health insurance. It created jobs and left no one behind.',\n  'result': \" The American Rescue Plan is a $2 Trillion tax cut passed in the previous administration that helped working people and left no one behind. It created jobs, boosted economic growth, delivered immediate economic relief for tens of millions of Americans, put food on their table, kept a roof over their heads, and cut the cost of health insurance. It also provided investments and tax credits to weatherize homes and businesses to be energy efficient, doubled America's clean energy production, lowered the price of electric vehicles, increased funding for prevention, treatment, harm reduction, and recovery for addiction, provided schools with money to hire teachers and help students make up for lost learning, and supported efforts to combat bullying, violence, trauma, and the harms of social media.\",\n  'grade': ' INCORRECT'},\n {'question': 'What is the purpose of the Bipartisan Innovation Act mentioned in the text?',\n  'answer': 'The Bipartisan Innovation Act will make record investments in emerging technologies and American manufacturing to level the playing field with China and other competitors.',\n  'result': ' The purpose of the Bipartisan Innovation Act is to make record investments in emerging technologies and American manufacturing.',\n  'grade': ' CORRECT'},\n {'question': \"What is Joe Biden's plan to fight inflation?\",\n  'answer': \"Joe Biden's plan to fight inflation is to lower costs, not wages, by making more goods in America, increasing the productive capacity of the economy, and cutting the cost of prescription drugs, energy, and child care.\",\n  'result': \" Joe Biden's plan to fight inflation involves lowering costs, not wages, making more cars and semiconductors in America, investing in infrastructure and innovation in America, improving goods transportation in America, creating more jobs with good wages in America, and avoiding reliance on foreign supply chains. He also proposes cutting the cost of prescription drugs and closing loopholes so the wealthy pay the same tax rate as teachers and firefighters. He also wants to confirm his Federal Reserve nominees to help fight inflation.\",\n  'grade': ' CORRECT'},\n {'question': 'What is the proposed minimum tax rate for corporations under the plan?',\n  'answer': 'The proposed minimum tax rate for corporations is 15%.',\n  'result': ' 15%',\n  'grade': ' CORRECT'},\n {'question': 'What are the four common sense steps that the author suggests to move forward safely?',\n  'answer': 'The four common sense steps suggested by the author to move forward safely are: stay protected with vaccines and treatments, prepare for new variants, end the shutdown of schools and businesses, and stay vigilant.',\n  'result': ' The four common sense steps that the author suggests to move forward safely are: 1) Stay protected with vaccines and treatments; 2) Take on mental health; 3) Crack down on gun trafficking and ghost guns; and 4) Pass the PRO Act.',\n  'grade': ' INCORRECT'},\n {'question': 'What is the purpose of the American Rescue Plan?',\n  'answer': 'The purpose of the American Rescue Plan is to provide $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption.',\n  'result': ' The purpose of the American Rescue Plan is to provide economic relief for tens of millions of Americans, fuel efforts to vaccinate the nation and combat COVID-19, and provide funding for prevention, treatment, harm reduction, and recovery.',\n  'grade': ' INCORRECT'},\n {'question': 'What measures does the speaker ask Congress to pass to reduce gun violence?',\n  'answer': 'The speaker asks Congress to pass universal background checks, ban assault weapons and high-capacity magazines, and repeal the liability shield that makes gun manufacturers the only industry in America that can’t be sued.',\n  'result': \" The speaker asks Congress to pass universal background checks, ban assault weapons and high-capacity magazines, and repeal the liability shield that makes gun manufacturers the only industry in America that can't be sued.\",\n  'grade': ' CORRECT'},\n {'question': 'What is the Unity Agenda for the Nation that the President is offering?',\n  'answer': 'The Unity Agenda for the Nation includes four big things that can be done together: beat the opioid epidemic, take on mental health, support veterans, and strengthen the Violence Against Women Act.',\n  'result': ' The Unity Agenda for the Nation is four big things that the President believes we can do together: beat the opioid epidemic, protect freedom and liberty, expand fairness and opportunity, and save democracy. He also plans to modernize roads, airports, ports, and waterways, build a national network of 500,000 electric vehicle charging stations, replace poisonous lead pipes, provide affordable high-speed internet, and fix over 65,000 miles of highway and 1,500 bridges.',\n  'grade': ' INCORRECT'},\n {'question': 'What is the purpose of ARPA-H?',\n  'answer': 'ARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more.',\n  'result': \" The purpose of ARPA-H is to drive breakthroughs in cancer, Alzheimer's, diabetes, and more.\",\n  'grade': ' CORRECT'}]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T17:40:51.519944Z",
     "end_time": "2023-04-23T17:40:51.569376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_accuracy_score(\"What is the name of the cognac house that makes the main ingredient in The Hennchata?\", \"Hennessy\", \"FUCK\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T17:43:31.999457Z",
     "end_time": "2023-04-23T17:43:32.529902Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "QAEvalChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fc265b082e0>, verbose=False, prompt=PromptTemplate(input_variables=['query', 'result', 'answer'], output_parser=None, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score it as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nPlease remember to grade them based on being factually accurate. Begin!\\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7fc265b082e0>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False), output_key='text')"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_chain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T16:03:27.998882Z",
     "end_time": "2023-04-23T16:03:28.002034Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'question': 'What is the purpose of the NATO Alliance?',\n  'answer': 'The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.',\n  'result': ' The purpose of the NATO Alliance is to secure peace and stability in Europe after World War 2.',\n  'grade': ' CORRECT'},\n {'question': 'What is the U.S. Department of Justice doing to combat the crimes of Russian oligarchs?',\n  'answer': 'The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.',\n  'result': \" The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs. They are also joining with European allies to find and seize the oligarchs' yachts, luxury apartments, and private jets, and going after criminals who stole billions in relief money meant for small businesses and millions of Americans.\",\n  'grade': ' INCORRECT'},\n {'question': 'What is the American Rescue Plan and how did it help Americans?',\n  'answer': 'The American Rescue Plan is a piece of legislation that provided immediate economic relief for tens of millions of Americans. It helped put food on their table, keep a roof over their heads, and cut the cost of health insurance. It created jobs and left no one behind.',\n  'result': \" The American Rescue Plan is a $2 Trillion tax cut passed in the previous administration that helped working people and left no one behind. It created jobs, boosted economic growth, delivered immediate economic relief for tens of millions of Americans, put food on their table, kept a roof over their heads, and cut the cost of health insurance. It also provided investments and tax credits to weatherize homes and businesses to be energy efficient, doubled America's clean energy production, lowered the price of electric vehicles, increased funding for prevention, treatment, harm reduction, and recovery for addiction, provided schools with money to hire teachers and help students make up for lost learning, and supported efforts to combat bullying, violence, trauma, and the harms of social media.\",\n  'grade': ' INCORRECT'},\n {'question': 'What is the purpose of the Bipartisan Innovation Act mentioned in the text?',\n  'answer': 'The Bipartisan Innovation Act will make record investments in emerging technologies and American manufacturing to level the playing field with China and other competitors.',\n  'result': ' The purpose of the Bipartisan Innovation Act is to make record investments in emerging technologies and American manufacturing.',\n  'grade': ' CORRECT'},\n {'question': \"What is Joe Biden's plan to fight inflation?\",\n  'answer': \"Joe Biden's plan to fight inflation is to lower costs, not wages, by making more goods in America, increasing the productive capacity of the economy, and cutting the cost of prescription drugs, energy, and child care.\",\n  'result': \" Joe Biden's plan to fight inflation involves lowering costs, not wages, making more cars and semiconductors in America, investing in infrastructure and innovation in America, improving goods transportation in America, creating more jobs with good wages in America, and avoiding reliance on foreign supply chains. He also proposes cutting the cost of prescription drugs and closing loopholes so the wealthy pay the same tax rate as teachers and firefighters. He also wants to confirm his Federal Reserve nominees to help fight inflation.\",\n  'grade': ' CORRECT'},\n {'question': 'What is the proposed minimum tax rate for corporations under the plan?',\n  'answer': 'The proposed minimum tax rate for corporations is 15%.',\n  'result': ' 15%',\n  'grade': ' CORRECT'},\n {'question': 'What are the four common sense steps that the author suggests to move forward safely?',\n  'answer': 'The four common sense steps suggested by the author to move forward safely are: stay protected with vaccines and treatments, prepare for new variants, end the shutdown of schools and businesses, and stay vigilant.',\n  'result': ' The four common sense steps that the author suggests to move forward safely are: 1) Stay protected with vaccines and treatments; 2) Take on mental health; 3) Crack down on gun trafficking and ghost guns; and 4) Pass the PRO Act.',\n  'grade': ' INCORRECT'},\n {'question': 'What is the purpose of the American Rescue Plan?',\n  'answer': 'The purpose of the American Rescue Plan is to provide $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption.',\n  'result': ' The purpose of the American Rescue Plan is to provide economic relief for tens of millions of Americans, fuel efforts to vaccinate the nation and combat COVID-19, and provide funding for prevention, treatment, harm reduction, and recovery.',\n  'grade': ' INCORRECT'},\n {'question': 'What measures does the speaker ask Congress to pass to reduce gun violence?',\n  'answer': 'The speaker asks Congress to pass universal background checks, ban assault weapons and high-capacity magazines, and repeal the liability shield that makes gun manufacturers the only industry in America that can’t be sued.',\n  'result': \" The speaker asks Congress to pass universal background checks, ban assault weapons and high-capacity magazines, and repeal the liability shield that makes gun manufacturers the only industry in America that can't be sued.\",\n  'grade': ' CORRECT'},\n {'question': 'What is the Unity Agenda for the Nation that the President is offering?',\n  'answer': 'The Unity Agenda for the Nation includes four big things that can be done together: beat the opioid epidemic, take on mental health, support veterans, and strengthen the Violence Against Women Act.',\n  'result': ' The Unity Agenda for the Nation is four big things that the President believes we can do together: beat the opioid epidemic, protect freedom and liberty, expand fairness and opportunity, and save democracy. He also plans to modernize roads, airports, ports, and waterways, build a national network of 500,000 electric vehicle charging stations, replace poisonous lead pipes, provide affordable high-speed internet, and fix over 65,000 miles of highway and 1,500 bridges.',\n  'grade': ' INCORRECT'},\n {'question': 'What is the purpose of ARPA-H?',\n  'answer': 'ARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more.',\n  'result': \" The purpose of ARPA-H is to drive breakthroughs in cancer, Alzheimer's, diabetes, and more.\",\n  'grade': ' CORRECT'}]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T16:04:56.223994Z",
     "end_time": "2023-04-23T16:04:56.229046Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'wall_time': 5.717400312423706,\n 'input': 'Pibb Xtra and Pet, Inc., are made in which country?',\n 'output': 'USA.',\n 'planner_log': \"For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E (#E1, #E2, ...) that can be called by later tools. Available tools are: \\n\\nTools can be one of the following:\\nWikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\\nLLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\\n\\nBegin! Describe your plans with rich details.\\n\\nQuestion: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\\nPlan: Search for more information about Colorado orogeny.\\n#E1 = Wikipedia[Colorado orogeny]\\nPlan: Find out the area that eastern sector of the Colorado orogeny extends into.\\n#E2 = LLM[What is the name of the area that eastern sector of Colorado extends into? Given context: #E1]\\nPlan: Search for more information about the area.\\n#E3 = Wikipedia[#E2]\\nPlan: Find out the elevation range for the area.\\n#E4 = LLM[What is elevation range for the area #E2? Given context: #E3]\\n\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nPlan: Search for more information about Arthur's Magazine.\\n#E1 = Wikipedia[Arthur's Magazine]\\nPlan: Search for more information about First for Women.\\n#E2 = Wikipedia[First for Women]\\nPlan: Compare the two start dates and determine which magazine was started first.\\n#E3 = LLM[Which magazine was started first Arthur's Magazine or First for Women? Given context: #E1, #E2]\\nPibb Xtra and Pet, Inc., are made in which country?\\n\\nPlan: Search for more information about Pibb Xtra and Pet, Inc.\\n#E1 = Wikipedia[Pibb Xtra and Pet, Inc.]\\nPlan: Find out the country in which Pibb Xtra and Pet, Inc. are made.\\n#E2 = LLM[In which country are Pibb Xtra and Pet, Inc. made? Given context: #E1]\",\n 'worker_log': 'Plan: Search for more information about Pibb Xtra and Pet, Inc.\\nEvidence:\\nVault was a sweetened carbonated beverage that was released by The Coca-Cola Company in June 2005 and marketed until December 2011. It was touted as an artificially flavored hybrid energy soda. Coca-Cola was marketing Vault as a combination with the slogan \"Drinks like a soda, kicks like an energy drink,\" as well as \"The Taste. The Quench. The Kick.\" \"Get it done, and then some,\" \"Chug & Charge,\" and \"Get to it!\"\\nPlan: Find out the country in which Pibb Xtra and Pet, Inc. are made.\\nEvidence:\\nUSA.\\n',\n 'solver_log': 'Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\\n\\nPibb Xtra and Pet, Inc., are made in which country?\\nPlan: Search for more information about Pibb Xtra and Pet, Inc.\\nEvidence:\\nVault was a sweetened carbonated beverage that was released by The Coca-Cola Company in June 2005 and marketed until December 2011. It was touted as an artificially flavored hybrid energy soda. Coca-Cola was marketing Vault as a combination with the slogan \"Drinks like a soda, kicks like an energy drink,\" as well as \"The Taste. The Quench. The Kick.\" \"Get it done, and then some,\" \"Chug & Charge,\" and \"Get to it!\"\\nPlan: Find out the country in which Pibb Xtra and Pet, Inc. are made.\\nEvidence:\\nUSA.\\n\\nNow begin to solve the task or problem. Respond directly with no extra words.\\nPibb Xtra and Pet, Inc., are made in which country?\\nUSA.',\n 'tool_usage': {'LLM_token': 122},\n 'steps': 3,\n 'total_tokens': 935,\n 'token_cost': 0.0187,\n 'tool_cost': 0.0,\n 'total_cost': 0.0187}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = PWS_Base(fewshot=fewshots.HOTPOTQA_PWS_BASE)\n",
    "agent.run(\"Pibb Xtra and Pet, Inc., are made in which country?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T20:03:45.052297Z",
     "end_time": "2023-04-23T20:03:50.776926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Leonardo Wilhelm DiCaprio (, ; Italian: [diˈkaːprjo]; born November 11, 1974) is an American actor and film producer. Known for his work in biographical and period films, he is the recipient of numerous accolades, including an Academy Award, a British Academy Film Award and three Golden Globe Awards. As of 2019, his films have grossed over $7.2 billion worldwide, and he has been placed eight times in annual rankings of the world's highest-paid actors.\\nBorn in Los Angeles, DiCaprio began his career in the late 1980s by appearing in television commercials. In the early 1990s, he had recurring roles in various television shows, such as the sitcom Parenthood, and had his first major film part as author Tobias Wolff in This Boy's Life (1993). He received critical acclaim and his first Academy Award and Golden Globe Award nominations for his performance as a developmentally disabled boy in What's Eating Gilbert Grape (1993). DiCaprio achieved international stardom with the star-crossed romances Romeo + Juliet (1996) and Titanic (1997). After the latter became the highest-grossing film at the time, he reduced his workload for a few years. In an attempt to shed his image of a romantic hero, DiCaprio sought roles in other genres, including the 2002 crime dramas Catch Me If You Can and Gangs of New York; the latter marked the first of his many successful collaborations with director Martin Scorsese.\\nDiCaprio earned Golden Globe nominations for his performances in the biopic The Aviator (2004), the political thriller Blood Diamond (2006), the crime drama The Departed (2006) and the romantic drama Revolutionary Road (2008). In the 2010s, he made environmental documentaries and starred in several high-profile directors' successful projects, including the action thriller Inception (2010), the western Django Unchained (2012), the biopic The Wolf of Wall Street (2013), the survival drama The Revenant (2015)—for which he won the Academy Award for Best Actor—and the comedy-drama Once Upon a Time in Hollywood (2019).\\nDiCaprio is the founder of Appian Way Productions—a production company that has made some of his films and the documentary series Greensburg (2008–2010)—and the Leonardo DiCaprio Foundation, a nonprofit organization devoted to promoting environmental awareness. A United Nations Messenger of Peace, he regularly supports charitable causes. In 2005, he was named a Commander of the Ordre des Arts et des Lettres for his contributions to the arts, and in 2016, he appeared in Time magazine's 100 most influential people in the world. DiCaprio was voted one of the 50 greatest actors of all time in a 2022 readers' poll by Empire.\""
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nodes.Worker import WikipediaWorker\n",
    "worker = WikipediaWorker()\n",
    "worker.run(\"Leo Decaprio\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T20:12:31.922697Z",
     "end_time": "2023-04-23T20:12:32.651908Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'wall_time': 10.688284158706665,\n 'input': \"Which magazine was started first Arthur's Magazine or First for Women?\",\n 'output': \"Arthur's Home Magazine began in 1861, so it was started first.\",\n 'planner_log': \"For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E (#E1, #E2, ...) that can be called by later tools.\\n\\nTools can be one of the following:\\nLLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\\nGoogle[input]: Worker that searches results from Google. Useful when you need to find short and general answers about a specific topic. Input should be a search query.\\nCalculator[input]: A calculator that can compute arithmetic expressions. Useful when you need to perform math calculations. Input should be a mathematical expression\\n\\nBegin! Describe your plans with rich details.\\n\\nQuestion: What is Leo Dicaprio's girlfriend's age to the power of 0.34? \\nPlan: Find out the name of Leo Dicaprio's girlfriend.\\n#E1 = Google[name of Leo Dicaprio's girlfriend] \\nPlan: Find out the age of Leo Dicaprio's girlfriend.\\n#E2 = Google[age of #E1]\\nPlan: Calculate her age to the power of 0.34.\\n#E3 = Calculator[#E2^0.34] \\nWhich magazine was started first Arthur's Magazine or First for Women?\\nPlan: Find out the year of the first issue of Arthur's Magazine.\\n#E1 = Google[year of the first issue of Arthur's Magazine] \\nPlan: Find out the year of the first issue of First for Women.\\n#E2 = Google[year of the first issue of First for Women] \\nPlan: Compare the years and determine which magazine was started first.\\n#E3 = LLM[compare #E1 and #E2]\",\n 'worker_log': \"Plan: Find out the year of the first issue of Arthur's Magazine.\\nEvidence:\\nArthur's Home Magazine began in 1861, succeeding The Ladies' Home Magazine and continuing its volume numbering.\\nPlan: Find out the year of the first issue of First for Women.\\nEvidence:\\nShop current and back issues from First for Women magazine for the best natural health, diet and wellbeing advice for women over 45.\\nPlan: Compare the years and determine which magazine was started first.\\nEvidence:\\nArthur's Home Magazine began in 1861 and Shop First for Women magazine began later for women over 45.\\n\",\n 'solver_log': \"Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\\n\\nWhich magazine was started first Arthur's Magazine or First for Women?\\nPlan: Find out the year of the first issue of Arthur's Magazine.\\nEvidence:\\nArthur's Home Magazine began in 1861, succeeding The Ladies' Home Magazine and continuing its volume numbering.\\nPlan: Find out the year of the first issue of First for Women.\\nEvidence:\\nShop current and back issues from First for Women magazine for the best natural health, diet and wellbeing advice for women over 45.\\nPlan: Compare the years and determine which magazine was started first.\\nEvidence:\\nArthur's Home Magazine began in 1861 and Shop First for Women magazine began later for women over 45.\\n\\nNow begin to solve the task or problem. Respond directly with no extra words.\\nWhich magazine was started first Arthur's Magazine or First for Women?\\nArthur's Home Magazine began in 1861, so it was started first.\",\n 'tool_usage': {'Google': 2, 'LLM_token': 90},\n 'steps': 4,\n 'total_tokens': 744,\n 'token_cost': 0.01488,\n 'tool_cost': 0.02,\n 'total_cost': 0.03488}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Which magazine was started first Arthur's Magazine or First for Women?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T22:25:52.965911Z",
     "end_time": "2023-04-23T22:26:03.657473Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "agent = PWS_Extra(fewshot=fewshots.HOTPOTQA_PWS_EXTRA)\n",
    "response = agent.run(\"Ikivo Animator integrates with a software application developed by which company?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:38:51.173409Z",
     "end_time": "2023-04-24T00:38:59.781234Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'wall_time': 8.608613729476929,\n 'input': 'Ikivo Animator integrates with a software application developed by which company?',\n 'output': 'Hypermedia.',\n 'planner_log': \"For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E (#E1, #E2, ...) that can be called by later tools.\\n\\nTools can be one of the following:\\nWikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\\nGoogle[input]: Worker that searches results from Google. Useful when you need to find short and succinct answers about a specific topic. Input should be a search query.\\nCalculator[input]: A calculator that can compute arithmetic expressions. Useful when you need to perform math calculations. Input should be a mathematical expression\\nLLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\\n\\nBegin! Describe your plans with rich details.\\n\\nQuestion: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\\nPlan: Search for more information about Colorado orogeny.\\n#E1 = Wikipedia[Colorado orogeny]\\nPlan: Find out the area that eastern sector of the Colorado orogeny extends into.\\n#E2 = LLM[What is the name of the area that eastern sector of Colorado extends into? Given context: #E1]\\nPlan: Search for more information about the area.\\n#E3 = Wikipedia[#E2]\\nPlan: Find out the elevation range for the area.\\n#E4 = LLM[What is elevation range for the area #E2? Given context: #E3]\\nPlan: Check on Google to see if the elevation range is correct.\\n#E5 = Google[Is the elevation range for #E2 #E4?]\\n\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nPlan: Search for more information about Arthur's Magazine.\\n#E1 = Wikipedia[Arthur's Magazine]\\nPlan: Search for more information about First for Women.\\n#E2 = Wikipedia[First for Women]\\nPlan: Compare the two start dates and determine which magazine was started first.\\n#E3 = LLM[Which magazine was started first Arthur's Magazine or First for Women? Given context: #E1, #E2]\\nPlan: Check on Google for additional check.\\n#E4 = Google[Is #E3 the magazine that was started first among Arthur's Magazine and First for Women?]\\nIkivo Animator integrates with a software application developed by which company?\\n\\nPlan: Search for more information about Ikivo Animator.\\n#E1 = Wikipedia[Ikivo Animator]\\nPlan: Find out the company that developed Ikivo Animator.\\n#E2 = LLM[Which company developed Ikivo Animator? Given context: #E1]\\nPlan: Check on Google to see if the company is correct.\\n#E3 = Google[Is the company #E2 the one that developed Ikivo Animator?]\",\n 'worker_log': \"Plan: Search for more information about Ikivo Animator.\\nEvidence:\\nCould not find [Ikivo Animator]. Similar: ['Hypermedia']\\nPlan: Find out the company that developed Ikivo Animator.\\nEvidence:\\nHypermedia\\nPlan: Check on Google to see if the company is correct.\\nEvidence:\\nSoftware applications, such as Ikivo Animator and Inkscape, simplify the development of hypermedia content based on SVG. Embedded devices, such as the ...\\n\",\n 'solver_log': \"Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\\n\\nIkivo Animator integrates with a software application developed by which company?\\nPlan: Search for more information about Ikivo Animator.\\nEvidence:\\nCould not find [Ikivo Animator]. Similar: ['Hypermedia']\\nPlan: Find out the company that developed Ikivo Animator.\\nEvidence:\\nHypermedia\\nPlan: Check on Google to see if the company is correct.\\nEvidence:\\nSoftware applications, such as Ikivo Animator and Inkscape, simplify the development of hypermedia content based on SVG. Embedded devices, such as the ...\\n\\nNow begin to solve the task or problem. Respond directly with no extra words.\\nIkivo Animator integrates with a software application developed by which company?\\nHypermedia.\",\n 'tool_usage': {'LLM_token': 30, 'Google': 1},\n 'steps': 4,\n 'total_tokens': 937,\n 'token_cost': 0.018740000000000003,\n 'tool_cost': 0.01,\n 'total_cost': 0.02874}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:38:59.782547Z",
     "end_time": "2023-04-24T00:38:59.823651Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E (#E1, #E2, ...) that can be called by later tools.\n",
      "\n",
      "Tools can be one of the following:\n",
      "Wikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\n",
      "Google[input]: Worker that searches results from Google. Useful when you need to find short and succinct answers about a specific topic. Input should be a search query.\n",
      "Calculator[input]: A calculator that can compute arithmetic expressions. Useful when you need to perform math calculations. Input should be a mathematical expression\n",
      "LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.\n",
      "\n",
      "Begin! Describe your plans with rich details.\n",
      "\n",
      "Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
      "Plan: Search for more information about Colorado orogeny.\n",
      "#E1 = Wikipedia[Colorado orogeny]\n",
      "Plan: Find out the area that eastern sector of the Colorado orogeny extends into.\n",
      "#E2 = LLM[What is the name of the area that eastern sector of Colorado extends into? Given context: #E1]\n",
      "Plan: Search for more information about the area.\n",
      "#E3 = Wikipedia[#E2]\n",
      "Plan: Find out the elevation range for the area.\n",
      "#E4 = LLM[What is elevation range for the area #E2? Given context: #E3]\n",
      "Plan: Check on Google to see if the elevation range is correct.\n",
      "#E5 = Google[Is the elevation range for #E2 #E4?]\n",
      "\n",
      "Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "Plan: Search for more information about Arthur's Magazine.\n",
      "#E1 = Wikipedia[Arthur's Magazine]\n",
      "Plan: Search for more information about First for Women.\n",
      "#E2 = Wikipedia[First for Women]\n",
      "Plan: Compare the two start dates and determine which magazine was started first.\n",
      "#E3 = LLM[Which magazine was started first Arthur's Magazine or First for Women? Given context: #E1, #E2]\n",
      "Plan: Check on Google for additional check.\n",
      "#E4 = Google[Is #E3 the magazine that was started first among Arthur's Magazine and First for Women?]\n",
      "Ikivo Animator integrates with a software application developed by which company?\n",
      "\n",
      "Plan: Search for more information about Ikivo Animator.\n",
      "#E1 = Wikipedia[Ikivo Animator]\n",
      "Plan: Find out the company that developed Ikivo Animator.\n",
      "#E2 = LLM[Which company developed Ikivo Animator? Given context: #E1]\n",
      "Plan: Check on Google to see if the company is correct.\n",
      "#E3 = Google[Is the company #E2 the one that developed Ikivo Animator?]\n"
     ]
    }
   ],
   "source": [
    "print(response[\"planner_log\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:39:35.380407Z",
     "end_time": "2023-04-24T00:39:35.384149Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\n",
      "\n",
      "Ikivo Animator integrates with a software application developed by which company?\n",
      "Plan: Search for more information about Ikivo Animator.\n",
      "Evidence:\n",
      "Could not find [Ikivo Animator]. Similar: ['Hypermedia']\n",
      "Plan: Find out the company that developed Ikivo Animator.\n",
      "Evidence:\n",
      "Hypermedia\n",
      "Plan: Check on Google to see if the company is correct.\n",
      "Evidence:\n",
      "Software applications, such as Ikivo Animator and Inkscape, simplify the development of hypermedia content based on SVG. Embedded devices, such as the ...\n",
      "\n",
      "Now begin to solve the task or problem. Respond directly with no extra words.\n",
      "Ikivo Animator integrates with a software application developed by which company?\n",
      "Hypermedia.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"solver_log\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T00:40:11.215609Z",
     "end_time": "2023-04-24T00:40:11.262273Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
